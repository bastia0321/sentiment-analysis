# sentiment-analysis
Basically, I used a CNN based architecture. The novelty here is, none of them uses the exact embedding model from glove or tok2vector. An approximating layer is used to approximately embed each token, and therefore some robustness and the size of the model is quite tiny.

3 models are built and tested. 1 model totally from scratch, 2 models from pretraining. The 1st model with pretrained coefficients uses the a pretrained model "en_core_web_lg" in Spacy. The second model from pretraining is somehow tricky; firstly, I used the a tool called StarSpace embedded the token in review tests by according item categories. Then the starspace embeddings are used to train a tok2vec layer (the one approximately embed each token), the tok2vec layer is then used in the final sentiment classification model.

The target of the model is to predict the review ratings from 1 to 5. Here I used category cross entropy as the loss function. I used the bigger Amazon review dataset to train and the smaller one to do the test. Results are out of my expectation, that is, the model built from scratch actually performs the best in accuracy. There should be some explanation, one of them is: 1. "en_core_web_lg" are a comprehensive language model, so it doesn't provide enough resolution/granularity to the sentiment analysis on amazon review; 2. when I use the starspace to embed the review tokens, the item label contains too much noise, and perhaps I should prune the item category info so the embedings of the token could carry more token-meaning related info.

I am tuning a another model, a BiLstm based model. The model pass each sentence of a review to a sub-model, which returns a sentence-embedded vector, and a higher level model, which is also a BiLSTM based model, will use these sentence-embedded vectors to get the final result. This time, besides the change in the architecture, I will try 2 more things: 1. I will treat ratings as numeric, and use mean-square-error-loss. 2. I want to consider the number of users voted on the review as weights of each training sample.